<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>BayesOpt: Using the library</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">BayesOpt
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('usemanual.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Using the library </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#running">Running your own problems.</a></li>
<li class="level1"><a href="#basicparams">Basic parameter setup</a></li>
<li class="level1"><a href="#usage">API description</a><ul><li class="level2"><a href="#cusage">C usage</a></li>
<li class="level2"><a href="#cppusage">C++ usage</a></li>
<li class="level2"><a href="#pyusage">Python usage</a></li>
<li class="level2"><a href="#matusage">Matlab/Octave usage</a></li>
</ul>
</li>
<li class="level1"><a href="#params">Understanding the parameters</a><ul><li class="level2"><a href="#budgetpar">Budget parameters</a></li>
<li class="level2"><a href="#initpar">Initialization parameters</a></li>
<li class="level2"><a href="#logpar">Logging parameters</a></li>
<li class="level2"><a href="#critpar">Exploration/exploitation parameters</a></li>
<li class="level2"><a href="#surrpar">Surrogate model parameters</a><ul><li class="level3"><a href="#meanpar">Mean function parameters</a></li>
<li class="level3"><a href="#kernelpar">Kernel parameters</a></li>
<li class="level3"><a href="#hyperlearn">Hyperparameter learning</a></li>
</ul>
</li>
<li class="level2"><a href="#savflags">Load/Save data parameters</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><p>The library is intended to be both fast and clear for development and research. At the same time, it allows great level of customization and guarantees a high level of accuracy and numerical robustness.</p>
<h1><a class="anchor" id="running"></a>
Running your own problems.</h1>
<p>The best way to design your own problem is by following one of the examples. Basically, there are 3 steps that should be followed:</p><ul>
<li>Define the function to optimize.</li>
<li>Modify the parameters of the optimization process. In general, many problems can be solved with the default set of parameters, but some of them will require some tuning.<ul>
<li>The set of parameters and the default set can be found in <a class="el" href="parameters_8h.html" title="Parameter definitions. ">parameters.h</a>.</li>
<li>In general most users will need to modify only the parameters described in <a class="el" href="usemanual.html#basicparams">Basic parameter setup</a>.</li>
<li>Advanced users should read <a class="el" href="usemanual.html#params">Understanding the parameters</a> for a full description of the parameters.</li>
</ul>
</li>
<li>Set and run the corresponding optimizer (continuous, discrete, categorical, etc.). In this step, the corresponding restriction should be defined.<ul>
<li>Continuous optimization requires box constraints (upper and lower bounds).</li>
<li>Discrete optimization requires the set of discrete values.</li>
<li>Categorical optimization requires the number of categories per dimension.</li>
</ul>
</li>
</ul>
<hr/>
<h1><a class="anchor" id="basicparams"></a>
Basic parameter setup</h1>
<p>Many users will only need to change the following parameters. Advanced users should read <a class="el" href="usemanual.html#params">Understanding the parameters</a> for a full description of the parameters.</p>
<ul>
<li><b>n_iterations:</b> Number of iterations of BayesOpt. Each iteration corresponds with a target function evaluation. Curently, this is the only stopping criteria. In general, more evaluations result in higher precision [Default 190]</li>
<li><b>noise:</b> Observation noise/signal ratio. [Default 1e-6]<ul>
<li>For stochastic functions (if several evaluations of the same point produce different results) it should match as close as possible the variance of the noise with respect to the variance of the signal. Too much noise results in slow convergence while not enough noise might result in not converging at all.</li>
<li>For simulations and deterministic functions, it should be close to 0. However, to avoid numerical instability due to model inaccuracies, make it always greater than 0. For example, between 1e-10 and 1e-14.</li>
</ul>
</li>
</ul>
<p>If execution time is not an issue, accuracy might be improving modifying the following parameters.</p>
<ul>
<li><b>l_type:</b> Learning method for the kernel hyperparameters. Setting this parameter to L_MCMC uses a more robust learning method which might result in better accuracy, but the overall execution time will increase. [Default L_EMPIRICAL]</li>
<li><b>n_iter_relearn:</b> Number of iterations between re-learning kernel parameters. That is, kernel learning ocur 1 out of <em>n_iter_relearn</em> iterations. Ideally, the best precision is obtained when the kernel parameters are learned every iteration (n_iter_relearn=1). However, this  learning part is computationally expensive and implies a higher cost per iteration. If n_iter_relearn=0, then there is no relearning. [Default 50]</li>
<li><b>n_inner_iterations:</b> (only for continuous optimization) Maximum number of iterations (per dimension!) to optimize the acquisition function (criteria). That is, each iteration corresponds with a criterion evaluation. If the original problem is high dimensional or the result is needed with high precision, we might need to increase this value. [Default 500]</li>
</ul>
<hr/>
<h1><a class="anchor" id="usage"></a>
API description</h1>
<p>Here we show a brief summary of the different ways to use the library. Basically, there are two ways to use the library based on your coding style:</p>
<p>-Callback: The user sends a function pointer or handler to the optimizer, following a prototype. This method is available for C/C++, Python, Matlab and Octave.</p>
<p>-Inheritance: This is a more object oriented method and allows more flexibility. The user creates a module with his function, process, etc. This module inherits one of BayesOpt models, depending if the optimization is discrete or continuous, and overrides the <em>evaluateSample</em> method. This method is available only for C++ and Python.</p>
<h2><a class="anchor" id="cusage"></a>
C usage</h2>
<p>This interface is the most standard approach. Due to the large compatibility with C code with other languages it could also be used for other languages such as Fortran, Ada, etc.</p>
<p>The function to optimize must agree with the template provided in <a class="el" href="bayesopt_8h.html" title="BayesOpt wrapper for C interface. ">bayesopt.h</a></p>
<div class="fragment"><div class="line"><span class="keywordtype">double</span> my_function (<span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n, <span class="keyword">const</span> <span class="keywordtype">double</span> *x, <span class="keywordtype">double</span> *gradient, <span class="keywordtype">void</span> *func_data);</div></div><!-- fragment --><p>Note that the gradient has been included for future compatibility, although in the current implementation, it is not used. You can just ignore it or send a NULL pointer.</p>
<p>The parameters are defined in the <a class="el" href="structbopt__params.html" title="Configuration parameters. ">bopt_params</a> struct. The easiest way to set the parameters is to use </p><div class="fragment"><div class="line"><a class="code" href="structbopt__params.html">bopt_params</a> initialize_parameters_to_default(<span class="keywordtype">void</span>);</div></div><!-- fragment --><p> and then, modify the necessary fields. For the non-numeric parameters, there are a set of functions that can help to set the corresponding parameters: </p><div class="fragment"><div class="line"><span class="keywordtype">void</span> set_kernel(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_mean(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_criteria(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_surrogate(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_log_file(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_load_file(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_save_file(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_learning(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div><div class="line"><span class="keywordtype">void</span> set_score(<a class="code" href="structbopt__params.html">bopt_params</a>* params, <span class="keyword">const</span> <span class="keywordtype">char</span>* name);</div></div><!-- fragment --><p> Basically, it just need a pointer to the parameters and a string for the parameter value. For example: </p><div class="fragment"><div class="line"><a class="code" href="structbopt__params.html">bopt_params</a> params = initialize_parameters_to_default();</div><div class="line">set_learning(&amp;params,<span class="stringliteral">&quot;L_MCMC&quot;</span>);</div></div><!-- fragment --><p>Once we have set the parameters and the function, we can called the optimizer according to our problem.</p>
<p>-For the continuous case: </p><div class="fragment"><div class="line"><span class="keywordtype">int</span> <a class="code" href="group__BayesOpt.html#ga345c3b2560085efeb726610f4d914ce4">bayes_optimization</a>(<span class="keywordtype">int</span> nDim, <span class="comment">// number of dimensions </span></div><div class="line">                       eval_func f, <span class="comment">// function to optimize </span></div><div class="line">                       <span class="keywordtype">void</span>* f_data, <span class="comment">// extra data that is transferred directly to f </span></div><div class="line">                       <span class="keyword">const</span> <span class="keywordtype">double</span> *lb, <span class="keyword">const</span> <span class="keywordtype">double</span> *ub, <span class="comment">// bounds </span></div><div class="line">                       <span class="keywordtype">double</span> *x, <span class="comment">// out: minimizer </span></div><div class="line">                       <span class="keywordtype">double</span> *minf, <span class="comment">// out: minimum </span></div><div class="line">                       <a class="code" href="structbopt__params.html">bopt_params</a> parameters);</div></div><!-- fragment --><p>-For the discrete case: </p><div class="fragment"><div class="line"><span class="keywordtype">int</span> <a class="code" href="group__BayesOpt.html#gac60ca43a9ad0bb102438dbffddd1e228">bayes_optimization_disc</a>(<span class="keywordtype">int</span> nDim, <span class="comment">// number of dimensions </span></div><div class="line">                            eval_func f, <span class="comment">// function to optimize </span></div><div class="line">                            <span class="keywordtype">void</span>* f_data, <span class="comment">// extra data that is transferred directly to f </span></div><div class="line">                            <span class="keywordtype">double</span> *valid_x, <span class="keywordtype">size_t</span> n_points, <span class="comment">// set of discrete points</span></div><div class="line">                            <span class="keywordtype">double</span> *x, <span class="comment">// out: minimizer </span></div><div class="line">                            <span class="keywordtype">double</span> *minf, <span class="comment">// out: minimum </span></div><div class="line">                            <a class="code" href="structbopt__params.html">bopt_params</a> parameters);</div></div><!-- fragment --><p>-For the categorical case: </p><div class="fragment"><div class="line"><span class="keywordtype">int</span> <a class="code" href="group__BayesOpt.html#ga968cfc4ace9f52705594c58e52f573de">bayes_optimization_categorical</a>(<span class="keywordtype">int</span> nDim, <span class="comment">// number of dimensions </span></div><div class="line">                 eval_func f, <span class="comment">// function to optimize </span></div><div class="line">                 <span class="keywordtype">void</span>* f_data, <span class="comment">// extra data that is transferred directly to f </span></div><div class="line">                 <span class="keywordtype">int</span> *categories, <span class="comment">// array of size nDim with the number of categories per dim </span></div><div class="line">                 <span class="keywordtype">double</span> *x, <span class="comment">// out: minimizer </span></div><div class="line">                 <span class="keywordtype">double</span> *minf, <span class="comment">// out: minimum </span></div><div class="line">                 <a class="code" href="structbopt__params.html">bopt_params</a> parameters);</div></div><!-- fragment --><p>This interface catches all the expected exceptions and returns error codes for C compatibility.</p>
<h2><a class="anchor" id="cppusage"></a>
C++ usage</h2>
<p>Besides being able to use the library with the <a class="el" href="usemanual.html#cusage">C usage</a> from C++, we can also take advantage of the object oriented properties of the language.</p>
<p>This is the most straightforward and complete method to use the library. The object that must be optimized must inherit from one of the models defined in <a class="el" href="bayesopt_8hpp.html" title="BayesOpt main C++ interface. ">bayesopt.hpp</a>.</p>
<p>Then, we just need to override the virtual functions called <b>evaluateSample</b> which correspond to the function to be optimized.</p>
<p>Optionally, we can redefine <b>checkReachability</b> to declare nonlinear constrain (if a point is invalid, checkReachability should return  false and if it is valid,  true). Note that the latter feature is experimental. There is no convergence guarantees if used.</p>
<p>For example, with for a continuous problem, we will define our optimizer as: </p><div class="fragment"><div class="line"><span class="keyword">class </span>MyOptimization: <span class="keyword">public</span> ContinuousModel</div><div class="line">{</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  MyOptimization(<a class="code" href="structbopt__params.html">bopt_params</a> param):</div><div class="line">    ContinuosModel(input_dimension,param) </div><div class="line">  {</div><div class="line">     <span class="comment">// My constructor </span></div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keywordtype">double</span> evaluateSample( <span class="keyword">const</span> boost::numeric::ublas::vector&lt;double&gt; &amp;query ) </div><div class="line">  {</div><div class="line">     <span class="comment">// My function here</span></div><div class="line">  };</div><div class="line"></div><div class="line">  <span class="keywordtype">bool</span> checkReachability( <span class="keyword">const</span> boost::numeric::ublas::vector&lt;double&gt; &amp;query )</div><div class="line">  { </div><div class="line">     <span class="comment">// My restrictions here </span></div><div class="line">  };</div><div class="line">};</div></div><!-- fragment --><p>where  input_dimension is an size_t value with the number of input dimensions of the problem. Note that for C++ we use the <a class="el" href="classbayesopt_1_1Parameters.html">Parameters</a> class defined in <a class="el" href="parameters_8hpp.html" title="Parameter definitions. ">parameters.hpp</a> for convenience.</p>
<p>Then, we use it like: </p><div class="fragment"><div class="line">Parameters param;</div><div class="line">params.<a class="code" href="structbopt__params.html#ab6199f3d845f1c21441ba996f5628ee6">l_type</a> = L_MCMC;</div><div class="line"></div><div class="line">MyOptimization optimizer(params);</div><div class="line"></div><div class="line"><span class="comment">//Define bounds and prepare result.</span></div><div class="line">boost::numeric::ublas::vector&lt;double&gt; bestPoint(dim);</div><div class="line">boost::numeric::ublas::vector&lt;double&gt; lowerBound(dim);</div><div class="line">boost::numeric::ublas::vector&lt;double&gt; upperBound(dim);</div><div class="line"></div><div class="line"><span class="comment">//Set the bounds. This is optional. Default is [0,1]</span></div><div class="line"><span class="comment">//Only required because we are doing continuous optimization</span></div><div class="line">optimizer.setBoundingBox(lowerBounds,upperBounds);</div><div class="line"></div><div class="line"><span class="comment">//Collect the result in bestPoint</span></div><div class="line">optimizer.optimize(bestPoint);</div></div><!-- fragment --><p>For discrete a categorical cases, we just need to inherit from the <a class="el" href="classbayesopt_1_1DiscreteModel.html">DiscreteModel</a>. Depending on the type of input we can use the corresponding constructor. In this case, the setBoundingBox step should be skipped.</p>
<p>Optionally, we can also choose to run every iteration independently. See <a class="el" href="bayesopt_8hpp.html" title="BayesOpt main C++ interface. ">bayesopt.hpp</a> and <a class="el" href="bayesoptbase_8hpp.html" title="BayesOpt common module for interfaces. ">bayesoptbase.hpp</a></p>
<h2><a class="anchor" id="pyusage"></a>
Python usage</h2>
<p>The file <a class="el" href="demo__quad_8py_source.html">python/demo_quad.py</a> provides simple example of different ways to use the library from Python.</p>
<ol type="1">
<li><b><a class="el" href="classbayesopt_1_1Parameters.html">Parameters</a>:</b> The parameters are defined as a Python dictionary with the same structure and names as the <a class="el" href="classbayesopt_1_1Parameters.html">Parameters</a> class in the C++ interface, with the exception of <em>kernel</em>.* and <em>mean</em>.* which are replaced by <em>kernel_</em> and <em>mean_</em> respectively.</li>
</ol>
<p>There is no need to fill all the parameters. If any of the parameter is not included in the dictionary, the default value is included instead.</p>
<ol type="1">
<li>a) <b>Callback:</b> The callback interface is just a wrapper of the C interface. In this case, the callback function should have the prototype <div class="fragment"><div class="line"><span class="keyword">def </span>my_function (query):</div></div><!-- fragment --> where <em>query</em> is a numpy array and the function returns a double scalar.</li>
</ol>
<p>The optimization process for a continuous model can be called as </p><div class="fragment"><div class="line">y_out, x_out, error = bayesopt.optimize(my_function, </div><div class="line">              n_dimensions, </div><div class="line">              lower_bound, </div><div class="line">              upper_bound, </div><div class="line">              parameters)</div></div><!-- fragment --><p> where the result is a tuple with the minimum as a numpy array (x_out), the value of the function at the minimum (y_out) and an error code.</p>
<p>Analogously, the function for a discrete model is: </p><div class="fragment"><div class="line">y_out, x_out, error = bo.optimize_discrete(my_function, </div><div class="line">              x_set,</div><div class="line">              parameters)</div></div><!-- fragment --><p> where x_set is an array of arrays with the valid inputs.</p>
<p>And for the categorical case: </p><div class="fragment"><div class="line">y_out, x_out, error = bo.optimize_discrete(my_function, </div><div class="line">              categories,</div><div class="line">              parameters)</div></div><!-- fragment --><p> where categories is an integer array with the number of categories per dimension.</p>
<ol type="1">
<li>b) <b>Inheritance:</b> The object oriented methodology is similar to the C++ interface.</li>
</ol>
<div class="fragment"><div class="line"><span class="keyword">from</span> bayesoptmodule <span class="keyword">import</span> BayesOptContinuous</div><div class="line"></div><div class="line"><span class="keyword">class </span>MyOptimization(BayesOptContinuous):</div><div class="line">    <span class="keyword">def </span><a class="code" href="classbayesoptmodule_1_1BayesOptContinuous.html#a497802b0c5c06bbf33468836c5e47ef3">__init__</a>(self):</div><div class="line">        BayesOptContinuous.__init__(n_dimensions)</div><div class="line"></div><div class="line">    <span class="keyword">def </span>evaluateSample(self,query):</div><div class="line">        <span class="stringliteral">&quot;&quot;&quot; My function here &quot;&quot;&quot;</span></div></div><!-- fragment --><p>Then, the optimization process can be called as </p><div class="fragment"><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">my_opt = MyOptimization()</div><div class="line"></div><div class="line"><span class="comment"># Set non-default parameters</span></div><div class="line">params[<span class="stringliteral">&quot;l_type&quot;</span>] = <span class="stringliteral">&quot;L_MCMC&quot;</span></div><div class="line">my_opt.params = params</div><div class="line"></div><div class="line"><span class="comment"># Set the bounds. This is optional. Default is [0,1]</span></div><div class="line"><span class="comment"># Only required because we are doing continuous optimization</span></div><div class="line">my_opt.lower_bound = <span class="comment">#numpy array</span></div><div class="line">my_opt.upper_bound = <span class="comment">#numpy array</span></div><div class="line"></div><div class="line"><span class="comment"># Collect the results</span></div><div class="line">y_out, x_out, error = my_instance.optimize()</div></div><!-- fragment --><p> where the result is a tuple with the minimum as a numpy array (x_out), the value of the function at the minimum (y_out) and an error code.</p>
<p>For discrete a categorical cases, we just need to inherit from the <a class="el" href="classbayesoptmodule_1_1BayesOptDiscrete.html" title="Python Module for BayesOptDiscrete. ">bayesoptmodule.BayesOptDiscrete</a> or <a class="el" href="classbayesoptmodule_1_1BayesOptCategorical.html" title="Python Module for BayesOptCategorical. ">bayesoptmodule.BayesOptCategorical</a>. See <a class="el" href="bayesoptmodule_8py.html" title="BayesOpt wrapper for Python interface (OOP) ">bayesoptmodule.py</a>. In this case, the "set bounds" step should be skipped.</p>
<p>Note: For some "expected" error codes, a corresponding Python exception is raised. However, this exception is raised once the error code is found the Python environment, so it does not have track of any exception happening in the C++ part of the code.</p>
<h2><a class="anchor" id="matusage"></a>
Matlab/Octave usage</h2>
<p>The file matlab/runtest.m provides an example of different ways to use BayesOpt from Matlab/Octave.</p>
<p>The parameters are defined as a Matlab struct with the same structure and names as the <a class="el" href="structbopt__params.html" title="Configuration parameters. ">bopt_params</a> struct in the C/C++ interface, with the exception of <em>kernel</em>.* and <em>mean</em>.* which are replaced by <em>kernel_</em> and <em>mean_</em> respectively. Also, C arrays are replaced with vector, thus there is no need to set the number of elements as a separate entry.</p>
<p>There is no need to fill all the parameters. If any of the parameter is not included in the Matlab struct, the default value is automatically included instead.</p>
<p>The Matlab/Octave interface is just a wrapper of the C interface. In this case, the callback function should have the form </p><div class="fragment"><div class="line"><span class="keyword">function</span> y = my_function (query):</div></div><!-- fragment --><p> where <em>query</em> is a Matlab vector and the function returns a scalar value.</p>
<p>The optimization process can be run for continuous variables (both in Matlab and Octave) as </p><div class="fragment"><div class="line">[x_out, y_out] = bayesoptcont(<span class="stringliteral">&#39;my_function&#39;</span>, </div><div class="line">                        n_dimensions, </div><div class="line">                        parameters, </div><div class="line">                        lower_bound, </div><div class="line">                        upper_bound);</div></div><!-- fragment --><p> where the result is the minimum as a vector (x_out) and the value of the function at the minimum (y_out). Analogously, the optimization process for discrete variables: </p><div class="fragment"><div class="line">[x_out, y_out] = bayesoptdisc(<span class="stringliteral">&#39;my_function&#39;</span>, </div><div class="line">                              xset, </div><div class="line">                              parameters);</div></div><!-- fragment --><p> and for categorical variables: </p><div class="fragment"><div class="line">[x_out, y_out] = bayesoptcat(<span class="stringliteral">&#39;my_function&#39;</span>, </div><div class="line">                             categories, </div><div class="line">                             parameters);</div></div><!-- fragment --><p>In Matlab, but not in Octave, the optimization can also be called with function handlers. For example: </p><div class="fragment"><div class="line">[x_out, y_out] = bayesoptcont(@my_function, </div><div class="line">                        n_dimensions, </div><div class="line">                        parameters, </div><div class="line">                        lower_bound, </div><div class="line">                        upper_bound)</div></div><!-- fragment --> <hr/>
<h1><a class="anchor" id="params"></a>
Understanding the parameters</h1>
<p>BayesOpt relies on a complex and highly configurable mathematical model. In theory, it should work reasonably well for many problems in its default configuration. However, Bayesian optimization shines when we can include as much knowledge as possible about the target function or about the problem. Or, if the knowledge is not available, keep the model as general as possible (to avoid bias). In this part, knowledge about Gaussian processes or nonparametric models in general might be useful.</p>
<p>It is recommendable to read the page about <a class="el" href="bopttheory.html">Bayesian optimization</a> in advance.</p>
<p>The parameters are bundled in a structure (C/C++/Matlab/Octave) or dictionary (Python), depending on the API that we use. This is a brief explanation of every parameter.</p>
<h2><a class="anchor" id="budgetpar"></a>
Budget parameters</h2>
<p>This set of parameters have to deal with the number of evaluations or iterations for each step.</p>
<ul>
<li><b>n_iterations:</b> Number of iterations of BayesOpt. Each iteration corresponds with a target function evaluation. In general, more evaluations result in higher precision [Default 190]</li>
<li><b>n_iter_relearn:</b> Number of iterations between re-learning kernel parameters. That is, kernel learning ocur 1 out of <em>n_iter_relearn</em> iterations. Ideally, the best precision is obtained when the kernel parameters are learned every iteration (n_iter_relearn=1). However, this  learning part is computationally expensive and implies a higher cost per iteration. If n_iter_relearn=0, then there is no relearning. [Default 50]</li>
<li><b>n_inner_iterations:</b> (only for continuous optimization) Maximum number of iterations (per dimension!) to optimize the acquisition function (criteria). That is, each iteration corresponds with a criterion evaluation. If the original problem is high dimensional or the result is needed with high precision, we might need to increase this value. [Default 500]</li>
</ul>
<h2><a class="anchor" id="initpar"></a>
Initialization parameters</h2>
<p>Sometimes, BayesOpt requires an initial set of samples to learn a preliminary model of the target function. This parameter is important if n_iter_relearn is 0 or too high.</p>
<ul>
<li><b>n_init_samples:</b> Initial set of samples. Each sample requires a target function evaluation. [Default 10]</li>
<li><b>init_method:</b> (for continuous optimization only, unsigned integer) There are different strategies available for the initial design: [Default 1, LHS].<ol type="1">
<li>Latin Hypercube Sampling (LHS)</li>
<li>Sobol sequences</li>
<li>Uniform Sampling</li>
</ol>
</li>
</ul>
<p>Random numbers are used frequently, from initial design, to MCMC, Thompson sampling, etc. They are based on the boost random number library.</p>
<ul>
<li><b>random_seed:</b> If this value is positive (including 0), then it is used as a fixed seed for the boost random number generator. If the value is negative, a time based (variable) seed is used. For debugging or benchmarking purposes, it might be useful to freeze the random seed. [Default -1, variable seed].</li>
</ul>
<h2><a class="anchor" id="logpar"></a>
Logging parameters</h2>
<ul>
<li><b>verbose_level:</b> (integer)<ul>
<li>Negative -&gt; Error -&gt; stdout</li>
<li>0 -&gt; Warning -&gt; stdout</li>
<li>1 -&gt; Info -&gt; stdout</li>
<li>2 -&gt; Debug -&gt; stdout</li>
<li>3 -&gt; Warning -&gt; log file</li>
<li>4 -&gt; Info -&gt; log file</li>
<li>5 -&gt; Debug -&gt; log file</li>
<li>&gt;5 -&gt; Error -&gt; log file</li>
</ul>
</li>
<li><b>log_filename:</b> Name/path of the log file (if applicable, verbose_level&gt;=3) [Default "bayesopt.log"]</li>
</ul>
<h2><a class="anchor" id="critpar"></a>
Exploration/exploitation parameters</h2>
<p>This is the set of parameters that drives the sampling procedure to explore more unexplored regions or improve the best current result.</p>
<ul>
<li><b>crit_name:</b> Name of the sample selection criterion or a combination of them. It is used to select which points to evaluate for each iteration of the optimization process. Could be a combination of functions like "cHedge(cEI,cLCB,cPOI,cThompsonSampling)". See section <a class="el" href="bopttheory.html#critmod">Selection criteria</a> for the different possibilities. [Default: "cEI"]</li>
<li><b>crit_params</b>, (n_crit_params): Array with the set of parameters for the selected criteria. If there are more than one criterion, the parameters are split among them according to the number of parameters required for each criterion. If the vector is empty or n_crit_params is 0, then the default parameters are selected for each criteria. [Default: crit_params = []]. For C, the array needs a size variable <b>n_crit_params</b>. In this case, default: n_crit_params = 0.</li>
<li><b>epsilon:</b> According to some authors <a class="el" href="citelist.html#CITEREF_Bull2011">[3]</a>, it is recommendable to include an epsilon-greedy strategy to achieve near optimal convergence rates. Epsilon is the probability of performing a random (blind) evaluation of the target function. Higher values implies forced exploration while lower values relies more on the exploration/exploitation policy of the criterion [Default 0.0 (epsilon-greedy disabled)]</li>
<li><b>force-jump</b>: Sometimes, specially when the number of initial points is too small, the learned model might be wrong and the optimization get stuck. Forced jumps measure the number of iterations where the difference between consecutive observations is smaller than the expected noise. Thus, we assume that any gain is pure noise and we could get more information somewhere else. This parameter sets the number of iterations with no gain before doing a random jump. If the parameter is 0, then this is disable. [Default 20]</li>
</ul>
<h2><a class="anchor" id="surrpar"></a>
Surrogate model parameters</h2>
<p>The main advantage of Bayesian optimization over other optimization model is the use of a surrogate model. These parameters allow to configure it. See Section <a class="el" href="bopttheory.html#surrmod">Surrogate models</a> for a detailed description.</p>
<ul>
<li><b>surr_name:</b> Name of the hierarchical surrogate function (nonparametric process and the hyperpriors on sigma and w). [Default "sGaussianProcess"]</li>
<li><b>noise:</b> Observation noise/signal ratio. [Default 1e-6]<ul>
<li>For stochastic functions (if several evaluations of the same point produce different results) it should match as close as possible the variance of the noise with respect to the variance of the signal. Too much noise results in slow convergence while not enough noise might result in not converging at all.</li>
<li>For simulations and deterministic functions, it should be close to 0. However, to avoid numerical instability due to model inaccuracies, make it always greater than 0. For example, between 1e-10 and 1e-14.</li>
</ul>
</li>
<li><b>sigma_s:</b> (only used for "sGaussianProcess" and "sGaussianProcessNormal") Known signal variance [Default 1.0]</li>
<li><b>alpha</b>, <b>beta:</b> (only used for "sStudentTProcessNIG") Inverse-Gamma prior hyperparameters (if applicable) [Default 1.0, 1.0]</li>
</ul>
<h3><a class="anchor" id="meanpar"></a>
Mean function parameters</h3>
<p>This set of parameters represents the mean function (or trend) of the surrogate model.</p>
<ul>
<li><b>mean.name:</b> Name of the mean function. Could be a combination of functions like "mSum(mConst, mLinear)". See Section <a class="el" href="bopttheory.html#parmod">Parametric (mean) functions</a> for the different possibilities. [Default: "mConst"]</li>
<li><b>mean.coef_mean</b>, <b>mean.coef_std:</b> Mean function coefficients as vectors/array. [Default: "coef_mean=[1.0], coef_std=[1000.0]"]<ul>
<li>If the mean function is assumed to be known (like in "sGaussianProcess"), then coef_mean represents the actual values and coef_std is ignored.</li>
<li>If the mean function has normal prior on the coeficients (like "sGaussianProcessNormal" or "sStudentTProcessNIG") then both the mean and std are used. The parameter mean.coef_std is a vector, it does not consider correlations.</li>
<li>For C, the size of both arrays is defined in <b>mean.n_coef</b>.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" id="kernelpar"></a>
Kernel parameters</h3>
<p>The kernel of the surrogate model represents the correlation between points, which is related to the smoothness of the prediction.</p>
<ul>
<li><b>kernel.name:</b> Name of the kernel function. Could be a combination of functions like "kSum(kSEARD,kMaternARD3)". See Section <a class="el" href="bopttheory.html#kermod">Kernel (covariance) models</a> for the different posibilities. [Default: "kMaternARD5"]</li>
<li><b>kernel.hp_mean</b>, <b>kernel.hp_std:</b> <a class="el" href="classbayesopt_1_1Kernel.html" title="Interface for kernel functors. ">Kernel</a> hyperparameters normal prior in the log space. That is, if the hyperparameters are <img class="formulaInl" alt="$\theta$" src="form_65.png"/>, this prior is <img class="formulaInl" alt="$p(\log(\theta))$" src="form_68.png"/>. Any "ilegal" standard deviation (std&lt;=0) results in a flat prior for the corresponding component. [Default:hp_mean=[1.0], hp_std=[10.0]]<ul>
<li>If there are more than one kernel (a compound kernel), the parameters are split among them according to the number of parameters required for each criterion.</li>
<li>ARD kernels require parameters for each dimension, if there are only one dimension provided (like in the default), it is copied for every dimension.</li>
<li>For C, the size of both arrays is defined in <b>kernel.n_hp</b>.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" id="hyperlearn"></a>
Hyperparameter learning</h3>
<p>Although BayesOpt tries to build a full analytic Bayesian model for the surrogate function, the kernel hyperparameters cannot be estimated in closed form. See Section <a class="el" href="bopttheory.html#learnmod">Methods for learning the kernel parameters</a> for a detailed description</p>
<ul>
<li><b>l_type:</b> Learning method for the kernel hyperparameters. Currently, L_FIXED, L_EMPIRICAL and L_MCMC are implemented [Default L_EMPIRICAL]</li>
<li><b>sc_type:</b> Score function for the learning method. [Default SC_MAP]</li>
<li><b>l_all:</b> If true, all the parameters are learned (mean, sigma, etc.) using the method defined in l_type. If false, only the kernel hyperparameters are directly learned. [Default false]</li>
</ul>
<h2><a class="anchor" id="savflags"></a>
Load/Save data parameters</h2>
<p>We can select to store or restore data in files, to continue an optimization without starting over. The data is stored in plan text files. Thus, by modifying the files, this method can be used to preload existing datapoints in the model.</p>
<ul>
<li><b>load_save_flag:</b> 1-Load data, 2-Save data, 3-Load and append data. Other values, no file saving or restore [Default 0]</li>
<li><b>load_filename</b>, <b>save_filename</b>; Filename to load/save data [Default "bayesopt.dat"] </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="reference.html">Reference Manual</a></li>
    <li class="footer">Generated on Fri Dec 20 2019 18:57:39 for BayesOpt by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
